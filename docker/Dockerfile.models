# Judex Model Service - GPU-optimized inference container
# Designed for Cloud Run GPU or GKE GPU nodes
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg \
    tesseract-ocr \
    libgl1 \
    libglib2.0-0 \
    libgomp1 \
    git \
    && rm -rf /var/lib/apt/lists/*

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONPATH=/app \
    HF_HOME=/models/hf \
    TRANSFORMERS_CACHE=/models/hf/transformers \
    TEMP_DIR=/tmp/judex \
    TORCH_HOME=/models/torch \
    PORT=8080

# Create directories
RUN mkdir -p /models/hf /models/hf/transformers /models/torch /tmp/judex

# Copy requirements for model service
COPY requirements-models.txt requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy model code
COPY app/models/ ./app/models/
COPY app/core/ ./app/core/
COPY model_service/ ./model_service/
COPY scripts/prefetch_models.py ./scripts/

# Prefetch models during build (speeds up cold starts significantly)
# Comment out for faster builds during development
RUN python scripts/prefetch_models.py || echo "Model prefetch completed with warnings"

# Expose port (Cloud Run uses PORT env var)
EXPOSE 8080

# Health check with longer timeout for GPU initialization
HEALTHCHECK --interval=60s --timeout=30s --start-period=120s --retries=5 \
    CMD python -c "import os; import urllib.request; urllib.request.urlopen(f'http://localhost:{os.environ.get(\"PORT\", 8080)}/health', timeout=10)" || exit 1

# Run model service - use shell form to expand PORT env var
CMD uvicorn model_service.main:app --host 0.0.0.0 --port ${PORT:-8080}
